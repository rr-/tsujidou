#!/usr/bin/env python3
import pickle
from pathlib import Path
from typing import List, Tuple, Generator, Callable
from lib import engine, script
from lib.tlg import tlg
from lib.snapshot import Snapshot
from lib.open_ext import open_ext
import configargparse


Transformer = Callable[[Snapshot], bytes]


def image_transformer(snapshot: Snapshot) -> bytes:
    if ('png' in snapshot.extra_artifacts
            and snapshot.extra_artifacts['png'].was_changed):
        png_content = snapshot.read_extra_artifact('png')
        assert png_content is not None
        if 'meta' in snapshot.extra_artifacts:
            metadata = pickle.loads(snapshot.read_extra_artifact('meta'))
            assert metadata is not None
            return tlg.png_to_tlg(png_content, metadata)
        return png_content

    ret = snapshot.read_main_artifact()
    assert ret is not None
    return ret


def script_transformer(snapshot: Snapshot) -> bytes:
    script_content = snapshot.read_extra_artifact('script')
    assert script_content is not None
    return script.encode_script(script_content)


def filter_snapshots(
        snapshots: List[Snapshot],
        only_new: bool
) -> Generator[Snapshot, None, None]:
    for snapshot in snapshots:
        if not snapshot.entry.is_extractable:
            continue
        for artifact in snapshot.all_artifacts:
            if not artifact.path.exists():
                raise ValueError('File {} was deleted!'.format(artifact.path))
        if only_new and not snapshot.was_changed:
            continue
        yield snapshot


def pack_archive(
        target_path: Path,
        snapshots: List[Snapshot],
        transformer: Transformer) -> Generator[Snapshot, None, None]:
    snapshots = list(sorted(
        filter_snapshots(snapshots, only_new=False),
        key=lambda snapshot: snapshot.entry.file_num))

    with open_ext(target_path, 'wb') as handle:
        # write dummy file table to reserve space
        table = engine.FileTable([snapshot.entry for snapshot in snapshots])
        engine.write_file_table(handle, table)

        # write and update entries
        for snapshot in snapshots:
            print('Packing {:016x} <- {}'.format(
                snapshot.entry.file_name_hash,
                [str(artifact.path) for artifact in snapshot.all_artifacts]))

            content = transformer(snapshot)
            engine.write_file_content(handle, snapshot.entry, content)
            yield snapshot

        # rewrite table, this time with correct sizes and offsets
        handle.seek(0)
        engine.write_file_table(handle, table)


def patch_archive(
        target_path: Path,
        snapshots: List[Snapshot],
        transformer: Transformer) -> Generator[Snapshot, None, None]:
    # read the existing file table
    with open_ext(target_path, 'rb') as handle:
        table = engine.read_file_table(
            handle,
            file_name_hash_map={
                engine.get_file_name_hash(str(snapshot.entry.file_name)):
                    str(snapshot.entry.file_name)
                for snapshot in snapshots
            })

    with open_ext(target_path, 'ab') as handle:
        assert handle.tell() > 0

        for snapshot in filter_snapshots(snapshots, only_new=True):
            # use entry inside the table rather than the one held by snapshot:
            # changes made to the entry by engine.write_file_content need to be
            # visible in the file table.
            table_entry = next(
                table_entry
                for table_entry in table.entries
                if table_entry.file_name_hash == snapshot.entry.file_name_hash)
            assert table_entry

            print('Packing {:016x} <- {}'.format(
                snapshot.entry.file_name_hash,
                [str(artifact.path) for artifact in snapshot.all_artifacts]))
            content = transformer(snapshot)
            engine.write_file_content(handle, table_entry, content)
            yield snapshot

    # rewrite table, this time with correct sizes and offsets
    with open_ext(target_path, 'r+b') as handle:
        assert handle.tell() == 0
        engine.write_file_table(handle, table)


def parse_args() -> configargparse.Namespace:
    parser = configargparse.ArgumentParser(
        default_config_files=['./config.ini'])
    parser.add('--game-dir', required=True)
    parser.add('--data-dir', required=True, default='./data')
    parser.add('--repack', action='store_true')
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    game_dir = Path(args.game_dir)
    data_dir = Path(args.data_dir)
    repack = args.repack  # type: bool

    directories = [
        ('script', 'script.dat', script_transformer),
        ('arc0', 'arc0.dat', image_transformer),
        ('arc1', 'arc1.dat', image_transformer),
        ('arc2', 'arc2.dat', image_transformer),
    ]  # type: List[Tuple[str, str, Transformer]]

    for source_name, target_name, transformer in directories:
        source_dir = data_dir.joinpath(source_name)
        target_path = game_dir.joinpath(target_name)

        snapshot_path = data_dir.joinpath(source_name + '-snapshot.dat')
        with snapshot_path.open('rb') as handle:
            snapshots = pickle.load(handle)

        if repack:
            print('Packing directory {} -> {}'.format(source_dir, target_path))
            updated_snapshots = pack_archive(
                target_path, snapshots, transformer)
        else:
            print(
                'Patching directory {} -> {}'.format(source_dir, target_path))
            updated_snapshots = patch_archive(
                target_path, snapshots, transformer)

        for snapshot in updated_snapshots:
            for artifact in snapshot.all_artifacts:
                artifact.update_stat()
        with snapshot_path.open('wb') as handle:
            pickle.dump(snapshots, handle)


if __name__ == '__main__':
    main()
