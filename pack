#!/usr/bin/env python3
import pickle
from pathlib import Path
from typing import List, Tuple, Generator, Callable
from lib import engine
from lib.tlg import tlg
from lib.snapshot import SnapshotEntry
from lib.open_ext import open_ext
import configargparse


Transformer = Callable[[bytes, SnapshotEntry], bytes]


def default_transformer(content: bytes, snapshot: SnapshotEntry) -> bytes:
    if snapshot.file_name is not None \
            and snapshot.file_name.endswith('.tlg') \
            and snapshot.metadata:
        # it's only needed to convert PNG back to TLG if it contains valuable
        # metadata; otherwise it's okay to leave it as PNG - game understands
        # these.
        return tlg.png_to_tlg(content, snapshot.metadata)
    return content


def script_transformer(content: bytes, snapshot: SnapshotEntry) -> bytes:
    return content.decode('utf-8').encode('cp932')


def collect_entries(
        source_dir: Path,
        snapshots: List[SnapshotEntry],
        only_new: bool
) -> Generator[Tuple[Path, SnapshotEntry, engine.FileEntry], None, None]:
    print('Collecting files')

    relative_path_to_snapshot_map = {
        snapshot_entry.relative_path: snapshot_entry
        for snapshot_entry in snapshots
    }

    for source_path in source_dir.glob('**/*'):
        if not source_path.is_file():
            continue

        relative_path = source_path.relative_to(source_dir)

        try:
            snapshot = relative_path_to_snapshot_map[relative_path]
        except KeyError:
            raise ValueError('Unexpected file: {}'.format(relative_path))

        if only_new and source_path.stat().st_mtime <= snapshot.stat.st_mtime:
            continue

        yield (
            source_path,
            snapshot,
            engine.FileEntry(
                file_type=snapshot.file_type,
                file_name_hash=snapshot.file_name_hash,
                file_name=snapshot.file_name,

                # computed by engine.write_file_content
                offset=None,
                size_compressed=None,
                size_original=None,

                is_extractable=True))


def pack_archive(
        source_dir: Path,
        target_path: Path,
        snapshots: List[SnapshotEntry],
        transformer: Transformer) -> None:
    items = list(sorted(
        collect_entries(source_dir, snapshots, only_new=False)))
    entries = [entry for _source_path, _snapshot, entry in items]
    with open_ext(target_path, 'wb') as handle:
        # write dummy file table to reserve space
        table = engine.FileTable(entries)
        engine.write_file_table(handle, table)

        # write and update entries
        for source_path, snapshot, entry in items:
            print('Packing file {} -> {:016x}'.format(
                source_path, entry.file_name_hash))

            with source_path.open('rb') as tmp_handle:
                content = tmp_handle.read()
                content = transformer(content, snapshot)

            engine.write_file_content(handle, entry, content)

        # rewrite table, this time with correct sizes and offsets
        handle.seek(0)
        engine.write_file_table(handle, table)


def patch_archive(
        source_dir: Path,
        target_path: Path,
        snapshots: List[SnapshotEntry],  # mutated
        transformer: Transformer) -> None:
    # read the existing file table
    with open_ext(target_path, 'rb') as handle:
        table = engine.read_file_table(
            handle,
            file_name_hash_map={
                engine.get_file_name_hash(str(snapshot.file_name)):
                    str(snapshot.file_name)
                for snapshot in snapshots
            })

    with open_ext(target_path, 'ab') as handle:
        assert handle.tell() > 0

        for source_path, snapshot, entry in collect_entries(
                source_dir, snapshots, only_new=True):
            print('Patching file {} -> {:016x}'.format(
                source_path, entry.file_name_hash))

            # use entry inside the table rather than the one produced by
            # collect_entries - the changes made to it by
            # engine.write_file_content need to be propagated back to the file
            # table.
            original_entry = next(
                original_entry
                for original_entry in table.entries
                if original_entry.file_name_hash == entry.file_name_hash)
            assert original_entry

            with source_path.open('rb') as tmp_handle:
                content = tmp_handle.read()
                content = transformer(content, snapshot)

            engine.write_file_content(handle, original_entry, content)

            snapshot.stat = source_path.stat()

    # rewrite table, this time with correct sizes and offsets
    with open_ext(target_path, 'r+b') as handle:
        assert handle.tell() == 0
        engine.write_file_table(handle, table)


def parse_args() -> configargparse.Namespace:
    parser = configargparse.ArgumentParser(
        default_config_files=['./config.ini'])
    parser.add('--game-dir', required=True)
    parser.add('--data-dir', required=True, default='./data')
    parser.add('--repack', action='store_true')
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    game_dir = Path(args.game_dir)
    data_dir = Path(args.data_dir)
    repack = args.repack  # type: bool

    directories = [
        ('script', 'script.dat', script_transformer),
        ('arc0', 'arc0.dat', default_transformer),
        ('arc1', 'arc1.dat', default_transformer),
        ('arc2', 'arc2.dat', default_transformer),
    ]  # type: List[Tuple[str, str, Transformer]]

    for source_name, target_name, transformer in directories:
        source_dir = data_dir.joinpath(source_name)
        target_path = game_dir.joinpath(target_name)

        snapshot_path = data_dir.joinpath(source_name + '-snapshot.dat')
        with snapshot_path.open('rb') as handle:
            snapshots = pickle.load(handle)

        if repack:
            print('Packing directory {} -> {}'.format(source_dir, target_path))
            pack_archive(source_dir, target_path, snapshots, transformer)
        else:
            print(
                'Patching directory {} -> {}'.format(source_dir, target_path))
            patch_archive(source_dir, target_path, snapshots, transformer)
            with snapshot_path.open('wb') as handle:
                pickle.dump(snapshots, handle)


if __name__ == '__main__':
    main()
